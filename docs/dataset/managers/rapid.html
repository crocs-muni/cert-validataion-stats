<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>cevast.dataset.managers.rapid API documentation</title>
<meta name="description" content="This module contains DatasetManager interface implementation of RAPID dataset type." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cevast.dataset.managers.rapid</code></h1>
</header>
<section id="section-intro">
<p>This module contains DatasetManager interface implementation of RAPID dataset type.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;This module contains DatasetManager interface implementation of RAPID dataset type.&#34;&#34;&#34;

import os
import logging
import multiprocessing as mp
from collections import OrderedDict
from datetime import datetime
from typing import Tuple
from cevast.certdb import CertDB
from .manager import DatasetManager, DatasetManagerTask
from ..parsers import RapidParser
from ..collectors import RapidCollector
from ..dataset import DatasetType, Dataset, DatasetState, DatasetParsingError

__author__ = &#39;Radim Podola&#39;

log = logging.getLogger(__name__)


class RapidDatasetManager(DatasetManager):
    &#34;&#34;&#34;DatasetManager interface implementation of RAPID dataset type.&#34;&#34;&#34;

    _CERT_NAME_SUFFIX = &#39;certs&#39;
    _HOSTS_NAME_SUFFIX = &#39;hosts&#39;
    _CHAINS_NAME_SUFFIX = &#39;chains&#39;
    _BROKEN_CHAINS_NAME_SUFFIX = &#39;broken_chains&#39;

    dataset_type = DatasetType.RAPID.name

    # TODO add date range
    # TODO make ports optional
    def __init__(self, repository: str, date: datetime.date = datetime.today().date(),
                 ports: Tuple[str] = (&#39;443&#39;,), cpu_cores: int = 1):
        self._repository = repository
        self._date = date
        self._ports = (ports,) if isinstance(ports, str) else ports
        self._cpu_cores = cpu_cores
        self.__date_id = date.strftime(&#39;%Y%m%d&#39;)
        self.__dataset_path_any_port = Dataset(self._repository, self.dataset_type, self.__date_id, None)
        log.info(&#39;RapidDatasetManager initialized with repository=%s, date=%s, ports=%s&#39;, repository, date, ports)

    def run(self, task_pipline: Tuple[Tuple[DatasetManagerTask, dict]]) -&gt; None:
        collected_datasets, parsed_datasets = None, None
        # Sort just to ensure valid sequence
        task_pipline = sorted(task_pipline, key=lambda x: x[0])
        log.info(&#39;Started with task pipeline %s&#39;, task_pipline)
        # Run tasks
        for task_item in task_pipline:
            task, params = task_item
            log.info(&#39;Run task %s with parameters: %s&#39;, task, params)
            # Runs collection TASK, collected datasets might be used in next task
            if task == DatasetManagerTask.COLLECT:
                collected_datasets = self.collect(**params)
                log.info(&#34;Collected datasets: %s&#34;, collected_datasets)

            # Runs analyzing TASK
            elif task == DatasetManagerTask.ANALYSE:
                pass  # Not implemented yet

            # Runs parsing TASK, parsed datasets might be used in next task
            elif task == DatasetManagerTask.PARSE:
                if collected_datasets:  # If some datasets were just collected in pipeline, use these
                    parsed_datasets = self.__parse(datasets=collected_datasets, **params)
                else:
                    parsed_datasets = self.parse(**params)
                log.info(&#34;Parsed datasets: %s&#34;, parsed_datasets)

            # Runs validation TASK
            elif task == DatasetManagerTask.VALIDATE:
                if parsed_datasets:
                    validated_datasets = self.__validate(datasets=parsed_datasets, **params)
                else:  # If some datasets were just parsed in pipeline, use these
                    validated_datasets = self.validate(**params)
                log.info(&#34;Validated datasets: %s&#34;, validated_datasets)
        log.info(&#34;Finished&#34;)

    def collect(self, api_key: str = None) -&gt; Tuple[Dataset]:
        log.info(&#39;Collecting started&#39;)
        collector = RapidCollector(api_key)
        download_dir = self.__dataset_path_any_port.path(DatasetState.COLLECTED)
        # Collect datasets
        collected = collector.collect(
            download_dir=download_dir, date=self._date, filter_ports=self._ports, filter_types=(&#39;hosts&#39;, &#39;certs&#39;)
        )
        # Remove duplicates (same datasets with e.g. different suffix)
        datasets = tuple(OrderedDict.fromkeys(map(Dataset.from_full_path, collected)))
        log.info(&#39;%d dataset were downloaded&#39;, len(datasets))
        log.info(&#39;Collecting finished&#39;)
        return datasets if datasets else None

    def analyse(self, methods: list = None) -&gt; str:
        raise NotImplementedError

    def parse(self, certdb: CertDB) -&gt; Tuple[Dataset]:
        log.info(&#39;Parsing started&#39;)
        # if not self._ports:
        #    self.__dataset_path_any_port.get(DatasetState.COLLECTED)
        # ...
        # else:
        datasets = self.__init_datasets()
        # Parse datasets
        parsed = self.__parse(certdb=certdb, datasets=datasets)
        log.info(&#39;Parsing finished&#39;)
        return parsed if parsed else None

    def validate(self, certdb: CertDB, validator: object, validator_cfg: dict) -&gt; Tuple[Dataset]:
        log.info(&#39;Validation started&#39;)
        datasets = self.__init_datasets()
        # Validate datasets
        validated = self.__validate(
            certdb=certdb, datasets=datasets, validator=validator, validator_cfg=validator_cfg
        )
        log.info(&#39;Validation finished&#39;)
        return validated if validated else None

    def __init_datasets(self) -&gt; Tuple[Dataset]:
        return tuple(Dataset(self._repository, self.dataset_type, self.__date_id, port) for port in self._ports)

    def __init_parser(self, dataset: Dataset) -&gt; RapidParser:
        certs_file = dataset.full_path(DatasetState.COLLECTED, self._CERT_NAME_SUFFIX, True)
        hosts_file = dataset.full_path(DatasetState.COLLECTED, self._HOSTS_NAME_SUFFIX, True)
        if certs_file and hosts_file:
            chain_file = dataset.full_path(DatasetState.PARSED, self._CHAINS_NAME_SUFFIX, physically=True)
            broken_file = dataset.full_path(DatasetState.PARSED, self._BROKEN_CHAINS_NAME_SUFFIX, physically=True)
            try:
                parser = RapidParser(certs_file, hosts_file, chain_file, broken_file)
                log.info(&#34;Will parse dataset: %s&#34;, dataset.static_filename)
                return parser
            except FileNotFoundError:
                log.exception(&#34;Collected dataset not found&#34;)
        return None

    def __parse(self, certdb: CertDB, datasets: Tuple[Dataset], store_log: bool = True) -&gt; Tuple[Dataset]:
        # First validate datasets and init parsers
        parsable, parsers = [], []
        for dataset in datasets:
            parser = self.__init_parser(dataset)
            if parser is not None:
                parsers.append(parser)
                parsable.append(dataset)
        # Parse and store certificates
        for parser in parsers:
            try:
                parser.store_certs(certdb)
            except (OSError, ValueError):
                log.exception(&#34;Error during certs dataset parsing -&gt; rollback&#34;)
                certdb.rollback()
                raise DatasetParsingError(&#34;Error during certs dataset parsing&#34;)
        # Now parse and store chains
        for parser in parsers:
            try:
                parser.store_chains(certdb)
                if store_log:
                    # Store dataset parsing log
                    parser.save_parsing_log(os.path.splitext(parser.chain_file)[0] + &#39;.log&#39;)
            except OSError:
                log.exception(&#34;Error during hosts dataset parsing -&gt; commit&#34;)
                certdb.commit()
                raise DatasetParsingError(&#34;Error during hosts dataset parsing&#34;)
        # Remove parsed datasets
        for dataset in parsable:
            dataset.delete(DatasetState.COLLECTED)
        return tuple(parsable) if parsable else None

    def __validate(self, datasets: Tuple[Dataset], certdb: CertDB,
                   validator: object, validator_cfg: dict) -&gt; Tuple[Dataset]:
        pool = mp.Pool(self._cpu_cores)
        jobs = []
        os.makedirs(&#34;test/data&#34;, exist_ok=True)

        validatable = []
        for dataset in datasets:
            chain_file = dataset.full_path(DatasetState.PARSED, self._CHAINS_NAME_SUFFIX, True)
            if chain_file:
                log.info(&#34;Will validate dataset: %s&#34;, dataset.static_filename)
                for host, chain in RapidParser.read_chains(chain_file):
                    #certdb.export()
                    #chain = [certdb.export(cert, &#34;test/data&#34;, False) for cert in chain]
                    log.info(&#34;host: %s&#34;, host)
                    jobs.append(pool.apply_async(validator, args=(chain, validator_cfg, certdb)))
                validatable.append(dataset)

                with open(dataset.full_path(&#34;VALIDATED&#34;, &#39;open_ssl&#39;, physically=True), &#39;w&#39;) as out:
                    for job in jobs:
                        out.write(job.get() + &#39;\n&#39;)
                    log.info(&#34;len(jobs): %s&#34;, len(jobs))

                    jobs = []

        pool.close()
        pool.join()
        import shutil
        shutil.rmtree(&#34;test/data&#34;)
        return tuple(validatable) if validatable else None
        # TODO make callback -&gt; Executor class (Validator)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cevast.dataset.managers.rapid.RapidDatasetManager"><code class="flex name class">
<span>class <span class="ident">RapidDatasetManager</span></span>
<span>(</span><span>repository: str, date: <method 'date' of 'datetime.datetime' objects> = datetime.date(2020, 6, 26), ports: Tuple[str] = ('443',), cpu_cores: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>DatasetManager interface implementation of RAPID dataset type.</p>
<p>Initialize Manager.
<code>repository</code> is dataset repository,
'date' is date,
'ports' is list of ports more specifying datasets,
'cpu_cores' is maximum number of CPU cores that might be used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RapidDatasetManager(DatasetManager):
    &#34;&#34;&#34;DatasetManager interface implementation of RAPID dataset type.&#34;&#34;&#34;

    _CERT_NAME_SUFFIX = &#39;certs&#39;
    _HOSTS_NAME_SUFFIX = &#39;hosts&#39;
    _CHAINS_NAME_SUFFIX = &#39;chains&#39;
    _BROKEN_CHAINS_NAME_SUFFIX = &#39;broken_chains&#39;

    dataset_type = DatasetType.RAPID.name

    # TODO add date range
    # TODO make ports optional
    def __init__(self, repository: str, date: datetime.date = datetime.today().date(),
                 ports: Tuple[str] = (&#39;443&#39;,), cpu_cores: int = 1):
        self._repository = repository
        self._date = date
        self._ports = (ports,) if isinstance(ports, str) else ports
        self._cpu_cores = cpu_cores
        self.__date_id = date.strftime(&#39;%Y%m%d&#39;)
        self.__dataset_path_any_port = Dataset(self._repository, self.dataset_type, self.__date_id, None)
        log.info(&#39;RapidDatasetManager initialized with repository=%s, date=%s, ports=%s&#39;, repository, date, ports)

    def run(self, task_pipline: Tuple[Tuple[DatasetManagerTask, dict]]) -&gt; None:
        collected_datasets, parsed_datasets = None, None
        # Sort just to ensure valid sequence
        task_pipline = sorted(task_pipline, key=lambda x: x[0])
        log.info(&#39;Started with task pipeline %s&#39;, task_pipline)
        # Run tasks
        for task_item in task_pipline:
            task, params = task_item
            log.info(&#39;Run task %s with parameters: %s&#39;, task, params)
            # Runs collection TASK, collected datasets might be used in next task
            if task == DatasetManagerTask.COLLECT:
                collected_datasets = self.collect(**params)
                log.info(&#34;Collected datasets: %s&#34;, collected_datasets)

            # Runs analyzing TASK
            elif task == DatasetManagerTask.ANALYSE:
                pass  # Not implemented yet

            # Runs parsing TASK, parsed datasets might be used in next task
            elif task == DatasetManagerTask.PARSE:
                if collected_datasets:  # If some datasets were just collected in pipeline, use these
                    parsed_datasets = self.__parse(datasets=collected_datasets, **params)
                else:
                    parsed_datasets = self.parse(**params)
                log.info(&#34;Parsed datasets: %s&#34;, parsed_datasets)

            # Runs validation TASK
            elif task == DatasetManagerTask.VALIDATE:
                if parsed_datasets:
                    validated_datasets = self.__validate(datasets=parsed_datasets, **params)
                else:  # If some datasets were just parsed in pipeline, use these
                    validated_datasets = self.validate(**params)
                log.info(&#34;Validated datasets: %s&#34;, validated_datasets)
        log.info(&#34;Finished&#34;)

    def collect(self, api_key: str = None) -&gt; Tuple[Dataset]:
        log.info(&#39;Collecting started&#39;)
        collector = RapidCollector(api_key)
        download_dir = self.__dataset_path_any_port.path(DatasetState.COLLECTED)
        # Collect datasets
        collected = collector.collect(
            download_dir=download_dir, date=self._date, filter_ports=self._ports, filter_types=(&#39;hosts&#39;, &#39;certs&#39;)
        )
        # Remove duplicates (same datasets with e.g. different suffix)
        datasets = tuple(OrderedDict.fromkeys(map(Dataset.from_full_path, collected)))
        log.info(&#39;%d dataset were downloaded&#39;, len(datasets))
        log.info(&#39;Collecting finished&#39;)
        return datasets if datasets else None

    def analyse(self, methods: list = None) -&gt; str:
        raise NotImplementedError

    def parse(self, certdb: CertDB) -&gt; Tuple[Dataset]:
        log.info(&#39;Parsing started&#39;)
        # if not self._ports:
        #    self.__dataset_path_any_port.get(DatasetState.COLLECTED)
        # ...
        # else:
        datasets = self.__init_datasets()
        # Parse datasets
        parsed = self.__parse(certdb=certdb, datasets=datasets)
        log.info(&#39;Parsing finished&#39;)
        return parsed if parsed else None

    def validate(self, certdb: CertDB, validator: object, validator_cfg: dict) -&gt; Tuple[Dataset]:
        log.info(&#39;Validation started&#39;)
        datasets = self.__init_datasets()
        # Validate datasets
        validated = self.__validate(
            certdb=certdb, datasets=datasets, validator=validator, validator_cfg=validator_cfg
        )
        log.info(&#39;Validation finished&#39;)
        return validated if validated else None

    def __init_datasets(self) -&gt; Tuple[Dataset]:
        return tuple(Dataset(self._repository, self.dataset_type, self.__date_id, port) for port in self._ports)

    def __init_parser(self, dataset: Dataset) -&gt; RapidParser:
        certs_file = dataset.full_path(DatasetState.COLLECTED, self._CERT_NAME_SUFFIX, True)
        hosts_file = dataset.full_path(DatasetState.COLLECTED, self._HOSTS_NAME_SUFFIX, True)
        if certs_file and hosts_file:
            chain_file = dataset.full_path(DatasetState.PARSED, self._CHAINS_NAME_SUFFIX, physically=True)
            broken_file = dataset.full_path(DatasetState.PARSED, self._BROKEN_CHAINS_NAME_SUFFIX, physically=True)
            try:
                parser = RapidParser(certs_file, hosts_file, chain_file, broken_file)
                log.info(&#34;Will parse dataset: %s&#34;, dataset.static_filename)
                return parser
            except FileNotFoundError:
                log.exception(&#34;Collected dataset not found&#34;)
        return None

    def __parse(self, certdb: CertDB, datasets: Tuple[Dataset], store_log: bool = True) -&gt; Tuple[Dataset]:
        # First validate datasets and init parsers
        parsable, parsers = [], []
        for dataset in datasets:
            parser = self.__init_parser(dataset)
            if parser is not None:
                parsers.append(parser)
                parsable.append(dataset)
        # Parse and store certificates
        for parser in parsers:
            try:
                parser.store_certs(certdb)
            except (OSError, ValueError):
                log.exception(&#34;Error during certs dataset parsing -&gt; rollback&#34;)
                certdb.rollback()
                raise DatasetParsingError(&#34;Error during certs dataset parsing&#34;)
        # Now parse and store chains
        for parser in parsers:
            try:
                parser.store_chains(certdb)
                if store_log:
                    # Store dataset parsing log
                    parser.save_parsing_log(os.path.splitext(parser.chain_file)[0] + &#39;.log&#39;)
            except OSError:
                log.exception(&#34;Error during hosts dataset parsing -&gt; commit&#34;)
                certdb.commit()
                raise DatasetParsingError(&#34;Error during hosts dataset parsing&#34;)
        # Remove parsed datasets
        for dataset in parsable:
            dataset.delete(DatasetState.COLLECTED)
        return tuple(parsable) if parsable else None

    def __validate(self, datasets: Tuple[Dataset], certdb: CertDB,
                   validator: object, validator_cfg: dict) -&gt; Tuple[Dataset]:
        pool = mp.Pool(self._cpu_cores)
        jobs = []
        os.makedirs(&#34;test/data&#34;, exist_ok=True)

        validatable = []
        for dataset in datasets:
            chain_file = dataset.full_path(DatasetState.PARSED, self._CHAINS_NAME_SUFFIX, True)
            if chain_file:
                log.info(&#34;Will validate dataset: %s&#34;, dataset.static_filename)
                for host, chain in RapidParser.read_chains(chain_file):
                    #certdb.export()
                    #chain = [certdb.export(cert, &#34;test/data&#34;, False) for cert in chain]
                    log.info(&#34;host: %s&#34;, host)
                    jobs.append(pool.apply_async(validator, args=(chain, validator_cfg, certdb)))
                validatable.append(dataset)

                with open(dataset.full_path(&#34;VALIDATED&#34;, &#39;open_ssl&#39;, physically=True), &#39;w&#39;) as out:
                    for job in jobs:
                        out.write(job.get() + &#39;\n&#39;)
                    log.info(&#34;len(jobs): %s&#34;, len(jobs))

                    jobs = []

        pool.close()
        pool.join()
        import shutil
        shutil.rmtree(&#34;test/data&#34;)
        return tuple(validatable) if validatable else None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="cevast.dataset.managers.manager.DatasetManager" href="manager.html#cevast.dataset.managers.manager.DatasetManager">DatasetManager</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="cevast.dataset.managers.manager.DatasetManager" href="manager.html#cevast.dataset.managers.manager.DatasetManager">DatasetManager</a></b></code>:
<ul class="hlist">
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.analyse" href="manager.html#cevast.dataset.managers.manager.DatasetManager.analyse">analyse</a></code></li>
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.collect" href="manager.html#cevast.dataset.managers.manager.DatasetManager.collect">collect</a></code></li>
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.dataset_type" href="manager.html#cevast.dataset.managers.manager.DatasetManager.dataset_type">dataset_type</a></code></li>
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.parse" href="manager.html#cevast.dataset.managers.manager.DatasetManager.parse">parse</a></code></li>
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.run" href="manager.html#cevast.dataset.managers.manager.DatasetManager.run">run</a></code></li>
<li><code><a title="cevast.dataset.managers.manager.DatasetManager.validate" href="manager.html#cevast.dataset.managers.manager.DatasetManager.validate">validate</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cevast.dataset.managers" href="index.html">cevast.dataset.managers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cevast.dataset.managers.rapid.RapidDatasetManager" href="#cevast.dataset.managers.rapid.RapidDatasetManager">RapidDatasetManager</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>